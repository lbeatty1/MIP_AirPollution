{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to impute emissions rates, then output a file that specifies emissions per location per MWh of generation\n",
    "The first challenge will be to get emissions rates (lb/mwh).  To do this I'll be taking EIA data on net generation and EPA data on emissions to calculate rates.  There will be many missings that I will fill in by taking capacity-weighted technology-planning_year means.\n",
    "\n",
    "Once I have emissions rates, the goal will be to output a series of shapefiles of location-specific emissions per MWh. A challenge here is assigning output to to specific plants.  I assign production based on capacity.\n",
    "\n",
    "### Brief Code Overview\n",
    "1. Data cleaning (formatting columns, names, etc)\n",
    "2. Merge EIA and EPA data into existing_gen_units using the EPA-EIA crosswalk. \n",
    "3. Impute generation for generators with missing generation.  To do this, I take plant-level generation and distribute that generation to generators based on each generator's capacity.\n",
    "4. Calculate generation-weighted average emissions rates, then use those to fill in generators with missing emissions rates (missings get filled in with a technology-planningyear weighted mean).\n",
    "5. Calculate aggregate emissions by multiplying rates by model-output specified generation.  As mentioned earlier, I calculate generator level generation either by calculating a generator's share of total capacity or net generation for that cluster-planningyear.\n",
    "6. Split the emissions output by planning year and write to shapefiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\1677484318.py:7: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "globals().clear()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "import math\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\15306011.py:5: DtypeWarning: Columns (17,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  existing_gen_units_2030 = pd.read_csv('MIP_results_comparison/case_settings/26-zone/usensys-inputs-short/usensys-inputs-short/2030/base_short_2030/extra_outputs/existing_gen_units.csv')\n",
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\15306011.py:6: DtypeWarning: Columns (17,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  existing_gen_units_2040 = pd.read_csv('MIP_results_comparison/case_settings/26-zone/usensys-inputs-short/usensys-inputs-short/2040/base_short_2040/extra_outputs/existing_gen_units.csv')\n",
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\15306011.py:7: DtypeWarning: Columns (17,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  existing_gen_units_2050 = pd.read_csv('MIP_results_comparison/case_settings/26-zone/usensys-inputs-short/usensys-inputs-short/2050/base_short_2050/extra_outputs/existing_gen_units.csv')\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:/Users/lbeatty/Documents/Lauren_MIP_Contribution/')\n",
    "\n",
    "\n",
    "#PG existing gen units output\n",
    "existing_gen_units_2030 = pd.read_csv('MIP_results_comparison/case_settings/26-zone/usensys-inputs-short/usensys-inputs-short/2030/base_short_2030/extra_outputs/existing_gen_units.csv')\n",
    "existing_gen_units_2040 = pd.read_csv('MIP_results_comparison/case_settings/26-zone/usensys-inputs-short/usensys-inputs-short/2040/base_short_2040/extra_outputs/existing_gen_units.csv')\n",
    "existing_gen_units_2050 = pd.read_csv('MIP_results_comparison/case_settings/26-zone/usensys-inputs-short/usensys-inputs-short/2050/base_short_2050/extra_outputs/existing_gen_units.csv')\n",
    "\n",
    "#PG generators data\n",
    "generators_data_2030 = pd.read_csv('MIP_results_comparison/case_settings/26-zone/usensys-inputs-short/usensys-inputs-short/2030/base_short_2030/Inputs/Generators_data.csv')\n",
    "generators_data_2040 = pd.read_csv('MIP_results_comparison/case_settings/26-zone/usensys-inputs-short/usensys-inputs-short/2030/base_short_2030/Inputs/Generators_data.csv')\n",
    "generators_data_2050 = pd.read_csv('MIP_results_comparison/case_settings/26-zone/usensys-inputs-short/usensys-inputs-short/2030/base_short_2030/Inputs/Generators_data.csv')\n",
    "\n",
    "#filter by retirement year\n",
    "#the code was written when existing_gen_units varied by year so lots of artifacts of the way the old output looked\n",
    "existing_gen_units_2030 = existing_gen_units_2030.query('retirement_year>=2030')\n",
    "existing_gen_units_2040 = existing_gen_units_2030.query('retirement_year>=2040')\n",
    "existing_gen_units_2050 = existing_gen_units_2030.query('retirement_year>=2050')\n",
    "\n",
    "\n",
    "#EIA-EPA Crosswalk\n",
    "crosswalk = pd.read_csv(\"Data/epa_eia_crosswalk.csv\")\n",
    "\n",
    "# EIA 860 for Generator Info\n",
    "eia860 = pd.read_excel(\"Data/eia860/3_1_Generator_Y2020.xlsx\", skiprows=1)\n",
    "\n",
    "# EIA923 for Generation Info\n",
    "eia923_fuels = pd.read_excel(\"Data/eia923/EIA923_Schedules_2_3_4_5_M_12_2020_Final_Revision.xlsx\", sheet_name='Page 1 Generation and Fuel Data', skiprows=5)\n",
    "eia923_generators = pd.read_excel(\"Data/eia923/EIA923_Schedules_2_3_4_5_M_12_2020_Final_Revision.xlsx\", sheet_name='Page 4 Generator Data', skiprows=5)\n",
    "\n",
    "#Emissions\n",
    "emissions = pd.read_csv(\"Data/CAMD/facilities_emissions.csv\")\n",
    "#only want year 2020\n",
    "emissions = emissions.query('year==2020')\n",
    "\n",
    "# PM25\n",
    "pm25 = pd.read_excel(\"Data/eGRID2020 DRAFT PM Emissions.xlsx\", sheet_name=\"2020 PM Unit-level Data\", skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\2972420735.py:11: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nei_cems = pd.read_csv('Data/NEI/egucems_SmokeFlatFile_2020NEI_POINT_20230128_27mar2023_v1.csv', skiprows=16)\n",
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\2972420735.py:12: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nei_noncems = pd.read_csv('Data/NEI/egunoncems_SmokeFlatFile_2020NEI_POINT_20230128_27mar2023_v0.csv', skiprows=15)\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "## Read in and format NEI ####\n",
    "###############################\n",
    "\n",
    "#nei data gives values for ammonia, vocs, and also pm2.5\n",
    "#It also has so2 and nox output that mostly matches camd data\n",
    "#I'll prioritise camd data, but fill in any missings with nei\n",
    "\n",
    "#cems noncems just distinguish whether emissions were continuously monitored or imputed\n",
    "#doesn't really matter for our purposes\n",
    "nei_cems = pd.read_csv('Data/NEI/egucems_SmokeFlatFile_2020NEI_POINT_20230128_27mar2023_v1.csv', skiprows=16)\n",
    "nei_noncems = pd.read_csv('Data/NEI/egunoncems_SmokeFlatFile_2020NEI_POINT_20230128_27mar2023_v0.csv', skiprows=15)\n",
    "nei = pd.concat([nei_cems, nei_noncems])\n",
    "\n",
    "#pull out the relevant pollutants\n",
    "nei = nei.groupby(['oris_facility_code', 'oris_boiler_id', 'poll']).agg({'ann_value': 'sum'}).reset_index()\n",
    "nei = nei[nei['poll'].isin(['CO', 'NH3', 'NOX', 'PM25-PRI', 'SO2', 'SO4', 'VOC'])]\n",
    "\n",
    "#put weights in pounds\n",
    "nei['poll']=nei['poll']+'_nei_tons'\n",
    "nei['ann_value']=nei['ann_value']\n",
    "nei = nei.pivot(index=['oris_facility_code', 'oris_boiler_id'], columns='poll', values='ann_value').reset_index()\n",
    "nei = nei.rename(columns = {'oris_facility_code': 'CAMD_PLANT_ID', 'oris_boiler_id': 'CAMD_UNIT_ID'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\3096755666.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  existing_gen_units_2040['plant_id_eia']=existing_gen_units_2040['plant_id_eia'].astype(int)\n",
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\3096755666.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  existing_gen_units_2050['plant_id_eia']=existing_gen_units_2050['plant_id_eia'].astype(int)\n",
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\3096755666.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  existing_gen_units_2040['generator_id']=existing_gen_units_2040['generator_id'].astype(str)\n",
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\3096755666.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  existing_gen_units_2050['generator_id']=existing_gen_units_2050['generator_id'].astype(str)\n",
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\3096755666.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  existing_gen_units_2040['planning_year']=2040\n",
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\3096755666.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  existing_gen_units_2050['planning_year']=2050\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "## Format columns ###\n",
    "#####################\n",
    "\n",
    "# existing_gen_units\n",
    "existing_gen_units_2030['plant_id_eia']=existing_gen_units_2030['plant_id_eia'].astype(int)\n",
    "existing_gen_units_2040['plant_id_eia']=existing_gen_units_2040['plant_id_eia'].astype(int)\n",
    "existing_gen_units_2050['plant_id_eia']=existing_gen_units_2050['plant_id_eia'].astype(int)\n",
    "\n",
    "existing_gen_units_2030['generator_id']=existing_gen_units_2030['generator_id'].astype(str)\n",
    "existing_gen_units_2040['generator_id']=existing_gen_units_2040['generator_id'].astype(str)\n",
    "existing_gen_units_2050['generator_id']=existing_gen_units_2050['generator_id'].astype(str)\n",
    "\n",
    "existing_gen_units_2030['planning_year']=2030\n",
    "existing_gen_units_2040['planning_year']=2040\n",
    "existing_gen_units_2050['planning_year']=2050\n",
    "\n",
    "#bind all into one\n",
    "existing_gen_units = pd.concat([existing_gen_units_2030, existing_gen_units_2040, existing_gen_units_2050])\n",
    "existing_gen_units = existing_gen_units.rename(columns={'generator_id': 'EIA_GENERATOR_ID', 'plant_id_eia':'EIA_PLANT_ID'})\n",
    "\n",
    "## EPA-EIA crosswalk\n",
    "crosswalk = crosswalk[['CAMD_PLANT_ID', 'CAMD_UNIT_ID', 'CAMD_GENERATOR_ID', 'EIA_PLANT_ID', 'EIA_GENERATOR_ID', 'EIA_UNIT_TYPE']]\n",
    "crosswalk['EIA_GENERATOR_ID'] = crosswalk['EIA_GENERATOR_ID'].astype(str)\n",
    "\n",
    "##############\n",
    "## EIA Data###\n",
    "##############\n",
    "\n",
    "# filter out AK and HI\n",
    "eia860 = eia860[~eia860['State'].isin(['HI', 'AK'])]\n",
    "eia923_fuels = eia923_fuels[~eia923_fuels['Plant State'].isin(['HI', 'AK'])]\n",
    "eia923_generators = eia923_generators[~eia923_generators['Plant State'].isin(['HI', 'AK'])]\n",
    "\n",
    "\n",
    "#rename columns and change formats\n",
    "eia860 = eia860[['Plant Code', 'Generator ID', 'Nameplate Capacity (MW)', 'Planned Retirement Year', 'Planned Retirement Month', 'Synchronized to Transmission Grid', 'Technology']]\n",
    "eia860.columns = ['EIA_PLANT_ID', 'EIA_GENERATOR_ID', 'Capacity', 'RetirementYear', 'RetirementMonth', 'SynchronizedToGrid', 'Technology']\n",
    "eia860['EIA_GENERATOR_ID']=eia860['EIA_GENERATOR_ID'].astype(str)\n",
    "\n",
    "eia923_fuels = eia923_fuels[['Plant Id', 'Plant Name', 'Plant State', 'Net Generation\\n(Megawatthours)']]\n",
    "eia923_fuels.columns = ['EIA_PLANT_ID', 'EIA_PLANT_NAME', 'EIA_STATE', 'NET_GEN_PLANT']\n",
    "eia923_fuels = eia923_fuels.groupby('EIA_PLANT_ID').agg({'NET_GEN_PLANT': 'sum'}).reset_index()\n",
    "\n",
    "eia923_generators = eia923_generators[['Plant Id', 'Generator Id', 'Net Generation\\nYear To Date']]\n",
    "eia923_generators.columns = ['EIA_PLANT_ID', 'EIA_GENERATOR_ID', 'NET_GEN_GENERATOR']\n",
    "eia923_generators['EIA_GENERATOR_ID'] = eia923_generators['EIA_GENERATOR_ID'].astype(str)\n",
    "\n",
    "##############\n",
    "##EPA data####\n",
    "##############\n",
    "pm25['UNITID'] = pm25['UNITID'].astype(str)\n",
    "pm25 = pm25.groupby(['ORISPL', 'UNITID']).agg({'PM25AN': 'sum'}).reset_index()\n",
    "pm25.columns = ['facilityId', 'unitId', 'pm25']\n",
    "pm25['unitId']=pm25['unitId'].astype(str)\n",
    "pm25['facilityId']=pm25['facilityId'].astype(int)\n",
    "\n",
    "emissions['unitId']=emissions['unitId'].astype(str)\n",
    "emissions['facilityId']=emissions['facilityId'].astype(int)\n",
    "\n",
    "emissions = pd.merge(emissions, pm25, on=['facilityId', 'unitId'], how='left')\n",
    "emissions['nox_tons'] = emissions['noxMass']\n",
    "emissions['so2_tons'] = emissions['so2Mass']\n",
    "emissions['pm25_tons'] = emissions['pm25']\n",
    "\n",
    "#rename columns\n",
    "emissions = emissions.rename(columns = {'facilityId': 'CAMD_PLANT_ID', 'unitId': 'CAMD_UNIT_ID'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merge EIA and EPA data into existing_gen_units using the EPA-EIA crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join existing_gen_units with crosswalk\n",
    "existing_gen_units = pd.merge(existing_gen_units, crosswalk, on=['EIA_GENERATOR_ID', 'EIA_PLANT_ID'],how='left')\n",
    "\n",
    "#in existing_gen_units, there's a fair amount of missing capacities that are available from eia data so I'm going to join in \n",
    "#eia860 and then fill in nans\n",
    "existing_gen_units = pd.merge(existing_gen_units, eia860[['EIA_GENERATOR_ID', 'EIA_PLANT_ID', 'Capacity']], on=['EIA_GENERATOR_ID', 'EIA_PLANT_ID'], how='left')\n",
    "\n",
    "#need net gen\n",
    "existing_gen_units = pd.merge(existing_gen_units, eia923_fuels, on=['EIA_PLANT_ID'], how='left')\n",
    "existing_gen_units = pd.merge(existing_gen_units, eia923_generators, on=['EIA_PLANT_ID', 'EIA_GENERATOR_ID'], how='left')\n",
    "\n",
    "#lastly, need emissions\n",
    "existing_gen_units = pd.merge(existing_gen_units, emissions, on=['CAMD_PLANT_ID', 'CAMD_UNIT_ID'], how='left')\n",
    "\n",
    "#nei emissions\n",
    "existing_gen_units = pd.merge(existing_gen_units, nei, on=['CAMD_PLANT_ID', 'CAMD_UNIT_ID'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in missing capacities\n",
    "existing_gen_units['capacity_mw']=existing_gen_units['capacity_mw'].combine_first(existing_gen_units['Capacity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Impute generation for generators with missing generation.  To do this, I take plant-level generation and distribute that generation to generators based on each generator's capacity.\n",
    "\n",
    "I'm going to calculate plant-level 'missing generation'\n",
    "Then divy up the missing generation to generators according to capacity in MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_plant_gen = existing_gen_units.groupby(['EIA_PLANT_ID', 'planning_year']).agg({'NET_GEN_GENERATOR': 'sum'}).reset_index()\n",
    "summed_plant_gen.columns=['EIA_PLANT_ID', 'planning_year', 'sum_generator_gen']\n",
    "\n",
    "existing_gen_units = pd.merge(existing_gen_units, summed_plant_gen, on=['EIA_PLANT_ID', 'planning_year'], how='left')\n",
    "\n",
    "existing_gen_units['missing_generator_generation']=existing_gen_units['NET_GEN_GENERATOR'].isna().astype(int)\n",
    "existing_gen_units['missing_generation'] = existing_gen_units['NET_GEN_PLANT'] - existing_gen_units['sum_generator_gen']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_generators = existing_gen_units.query('missing_generator_generation==1').\\\n",
    "    groupby(['EIA_PLANT_ID', 'planning_year']).\\\n",
    "    agg({'capacity_mw': 'sum'}).\\\n",
    "    reset_index()\n",
    "missing_generators.columns=['EIA_PLANT_ID', 'planning_year', 'missing_generator_capacity']\n",
    "\n",
    "existing_gen_units = pd.merge(existing_gen_units, missing_generators, on=['EIA_PLANT_ID', 'planning_year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make imputation\n",
    "\n",
    "existing_gen_units['pct_missing_capacity'] = (existing_gen_units['capacity_mw']/existing_gen_units['missing_generator_capacity'])*existing_gen_units['missing_generator_generation']\n",
    "existing_gen_units['imputed_net_gen'] = existing_gen_units['pct_missing_capacity']*existing_gen_units['missing_generation']\n",
    "\n",
    "\n",
    "#replace missings with imputed\n",
    "existing_gen_units['NET_GEN_GENERATOR'] = existing_gen_units['NET_GEN_GENERATOR'].combine_first(existing_gen_units['imputed_net_gen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate generation-weighted average emissions rates, then use those to fill in generators with missing emissions rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks like there's broad agreement between camd and nei\n",
    "#fill in missing camd data with nei data\n",
    "\n",
    "existing_gen_units['nox_tons'] = existing_gen_units['nox_tons'].combine_first(existing_gen_units['NOX_nei_tons'])\n",
    "existing_gen_units['so2_tons'] = existing_gen_units['so2_tons'].combine_first(existing_gen_units['SO2_nei_tons'])\n",
    "existing_gen_units['pm25_tons'] = existing_gen_units['pm25_tons'].combine_first(existing_gen_units['PM25-PRI_nei_tons'])\n",
    "\n",
    "#get rates\n",
    "existing_gen_units['nox_rate'] = existing_gen_units['nox_tons']/existing_gen_units['NET_GEN_GENERATOR']\n",
    "existing_gen_units['so2_rate'] = existing_gen_units['so2_tons']/existing_gen_units['NET_GEN_GENERATOR']\n",
    "existing_gen_units['pm25_rate'] = existing_gen_units['pm25_tons']/existing_gen_units['NET_GEN_GENERATOR']\n",
    "existing_gen_units['nh3_rate'] = existing_gen_units['NH3_nei_tons']/existing_gen_units['NET_GEN_GENERATOR']\n",
    "existing_gen_units['voc_rate'] = existing_gen_units['VOC_nei_tons']/existing_gen_units['NET_GEN_GENERATOR']\n",
    "\n",
    "\n",
    "\n",
    "#for now I'm going to omit units with negative net_gen since it doesn't make sense for them to have negative 'rates'\n",
    "#I'll replace the rates with sample weighted-means\n",
    "existing_gen_units.loc[existing_gen_units['nox_rate']<0, 'nox_rate']=np.nan\n",
    "existing_gen_units.loc[existing_gen_units['pm25_rate']<0, 'pm25_rate']=np.nan\n",
    "existing_gen_units.loc[existing_gen_units['so2_rate']<0, 'so2_rate']=np.nan\n",
    "existing_gen_units.loc[existing_gen_units['nh3_rate']<0, 'nh3_rate']=np.nan\n",
    "existing_gen_units.loc[existing_gen_units['voc_rate']<0, 'voc_rate']=np.nan\n",
    "\n",
    "existing_gen_units.loc[existing_gen_units['nox_rate'].isin([np.inf]), 'nox_rate']=np.nan\n",
    "existing_gen_units.loc[existing_gen_units['pm25_rate'].isin([np.inf]), 'pm25_rate']=np.nan\n",
    "existing_gen_units.loc[existing_gen_units['so2_rate'].isin([np.inf]), 'so2_rate']=np.nan\n",
    "existing_gen_units.loc[existing_gen_units['nh3_rate'].isin([np.inf]), 'nh3_rate']=np.nan\n",
    "existing_gen_units.loc[existing_gen_units['voc_rate'].isin([np.inf]), 'voc_rate']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## Calculate weighted average emissions rates by technology-planning year\n",
    "\n",
    "# Define a function to calculate weighted average handling NaN values\n",
    "\n",
    "def weighted_average(df):\n",
    "    weighted_avgs = {}\n",
    "    for col in df.columns:\n",
    "        if '_rate' in col:  # Consider columns containing '_rate'\n",
    "            df_valid = df.dropna(subset=[col, 'NET_GEN_GENERATOR'])\n",
    "            if len(df_valid) == 0 or df_valid['NET_GEN_GENERATOR'].sum() == 0:\n",
    "                weighted_avgs[col] = np.nan  # Return NaN if all weights in the group are zero\n",
    "            else:\n",
    "                weighted_avgs[col] = np.average(df_valid[col], weights=df_valid['NET_GEN_GENERATOR'])\n",
    "    return pd.Series(weighted_avgs)\n",
    "\n",
    "technology_rates = existing_gen_units.groupby(['technology', 'planning_year']).apply(weighted_average).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok last thing to do is fill in missings\n",
    "\n",
    "technology_rates = technology_rates[['technology', 'planning_year', 'nox_rate', 'pm25_rate', 'so2_rate', 'voc_rate', 'nh3_rate']]\n",
    "technology_rates.columns = ['technology', 'planning_year', 'noxrate_imputed', 'pm25rate_imputed', 'so2rate_imputed', 'vocrate_imputed', 'nh3rate_imputed']\n",
    "\n",
    "existing_gen_units = pd.merge(existing_gen_units, technology_rates, on=['technology', 'planning_year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_gen_units['noxrate_imputed'] = existing_gen_units['nox_rate'].combine_first(existing_gen_units['noxrate_imputed'])\n",
    "existing_gen_units['so2rate_imputed'] = existing_gen_units['so2_rate'].combine_first(existing_gen_units['so2rate_imputed'])\n",
    "existing_gen_units['pm25rate_imputed'] = existing_gen_units['pm25_rate'].combine_first(existing_gen_units['pm25rate_imputed'])\n",
    "existing_gen_units['vocrate_imputed'] = existing_gen_units['voc_rate'].combine_first(existing_gen_units['vocrate_imputed'])\n",
    "existing_gen_units['nh3rate_imputed'] = existing_gen_units['nh3_rate'].combine_first(existing_gen_units['nh3rate_imputed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nox_tons        764117.084000\n",
      "pm25_tons        88446.692134\n",
      "so2_tons        808305.497843\n",
      "NH3_nei_tons     18829.463402\n",
      "VOC_nei_tons     20387.986583\n",
      "dtype: float64\n",
      "nox_predicted     787127.484352\n",
      "so2_predicted     742088.506697\n",
      "pm25_predicted    100743.871080\n",
      "voc_predicted      24143.026328\n",
      "nh3_predicted      34291.854018\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "## Little detour here to calculate year 2020 emissions\n",
    "##This will serve as a gut-check and I'll output it so that in the results you can see the decline from 2020 --> 2030\n",
    "## First I'll just take nei and camd data and sum it.\n",
    "## Then I'll take observed net gen and multiply it by my imputed emissions rates\n",
    "## Hopefully emissions will be close\n",
    "\n",
    "reported_emissions_2020 = pd.merge(emissions, nei, how='outer', on=['CAMD_UNIT_ID', 'CAMD_PLANT_ID']).reset_index()\n",
    "reported_emissions_2020['nox_tons'] = reported_emissions_2020['nox_tons'].combine_first(existing_gen_units['NOX_nei_tons'])\n",
    "reported_emissions_2020['pm25_tons'] = reported_emissions_2020['pm25_tons'].combine_first(existing_gen_units['PM25-PRI_nei_tons'])\n",
    "reported_emissions_2020['so2_tons'] = reported_emissions_2020['so2_tons'].combine_first(existing_gen_units['SO2_nei_tons'])\n",
    "print(reported_emissions_2020[['nox_tons', 'pm25_tons', 'so2_tons', 'NH3_nei_tons', 'VOC_nei_tons']].sum())\n",
    "\n",
    "test_emissions_2020 = pd.merge(eia860[['EIA_GENERATOR_ID', 'EIA_PLANT_ID', 'Capacity', 'Technology']], eia923_generators, on=['EIA_GENERATOR_ID', 'EIA_PLANT_ID'], how='left')\n",
    "test_emissions_2020 = pd.merge(test_emissions_2020, eia923_fuels, on=['EIA_PLANT_ID'])\n",
    "pct_capacity_2020 = test_emissions_2020.groupby(['EIA_PLANT_ID']).agg({'Capacity':'sum'}).reset_index()\n",
    "pct_capacity_2020 = pct_capacity_2020.rename(columns={'Capacity':'sum_plant_capacity'})\n",
    "test_emissions_2020 = pd.merge(test_emissions_2020, pct_capacity_2020, how='left', on='EIA_PLANT_ID')\n",
    "test_emissions_2020['pct_capacity'] = test_emissions_2020['Capacity']/test_emissions_2020['sum_plant_capacity']\n",
    "test_emissions_2020['predicted_gen']=test_emissions_2020['pct_capacity']*test_emissions_2020['NET_GEN_PLANT']\n",
    "\n",
    "test_emissions_2020 = test_emissions_2020.rename(columns={'Technology':'technology'})\n",
    "test_emissions_2020 = pd.merge(test_emissions_2020, technology_rates[technology_rates['planning_year']==2030], how='left', on=['technology'])\n",
    "\n",
    "#replace the few negative net gens with zero\n",
    "test_emissions_2020.loc[test_emissions_2020['predicted_gen']<0, 'predicted_gen']=0\n",
    "test_emissions_2020 = test_emissions_2020[test_emissions_2020['predicted_gen']>0]\n",
    "\n",
    "test_emissions_2020['nox_predicted']=test_emissions_2020['predicted_gen']*test_emissions_2020['noxrate_imputed']\n",
    "test_emissions_2020['so2_predicted']=test_emissions_2020['predicted_gen']*test_emissions_2020['so2rate_imputed']\n",
    "test_emissions_2020['pm25_predicted']=test_emissions_2020['predicted_gen']*test_emissions_2020['pm25rate_imputed']\n",
    "test_emissions_2020['voc_predicted']=test_emissions_2020['predicted_gen']*test_emissions_2020['vocrate_imputed']\n",
    "test_emissions_2020['nh3_predicted']=test_emissions_2020['predicted_gen']*test_emissions_2020['nh3rate_imputed']\n",
    "\n",
    "\n",
    "test_emissions_2020 = test_emissions_2020[(test_emissions_2020['nox_predicted'] != 0) | (test_emissions_2020['so2_predicted'] != 0) | (test_emissions_2020['pm25_predicted'] != 0) | (test_emissions_2020['voc_predicted'] != 0) | (test_emissions_2020['nh3_predicted'] != 0)]\n",
    "#drop if all predicted emissions are 0\n",
    "print(test_emissions_2020[['nox_predicted', 'so2_predicted', 'pm25_predicted', 'voc_predicted', 'nh3_predicted']].sum())\n",
    "\n",
    "#looks pretty close to me\n",
    "#nh3 looks different but I think there's lots of missings\n",
    "#test_emissions_2020 is actually what I'm going to bind into emissions and analyze in InMap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate aggregate emissions by multiplying rates by model-output specified generation.  As mentioned earlier, I calculate generator level generation either by calculating a generator's share of total capacity or net generation for that cluster-planningyear.\n",
    "\n",
    "Start by specifying which model, scenario, and column you want outputs for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'capacity_mw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "technology_year_total = existing_gen_units.groupby(['Resource', 'planning_year']).agg({column : 'sum'}).reset_index()\n",
    "technology_year_total.columns = ['Resource', 'planning_year', 'technology_total']\n",
    "\n",
    "#merge in total capacity or total net gen\n",
    "existing_gen_units = pd.merge(existing_gen_units, technology_year_total, on=['Resource', 'planning_year'], how='left')\n",
    "\n",
    "\n",
    "existing_gen_units['pct_total']=existing_gen_units[column]/existing_gen_units['technology_total']\n",
    "\n",
    "existing_gen_units['nox_predicted']=existing_gen_units['pct_total']*existing_gen_units['noxrate_imputed']\n",
    "existing_gen_units['so2_predicted']=existing_gen_units['pct_total']*existing_gen_units['so2rate_imputed']\n",
    "existing_gen_units['pm25_predicted']=existing_gen_units['pct_total']*existing_gen_units['pm25rate_imputed']\n",
    "existing_gen_units['voc_predicted']=existing_gen_units['pct_total']*existing_gen_units['vocrate_imputed']\n",
    "existing_gen_units['nh3_predicted']=existing_gen_units['pct_total']*existing_gen_units['nh3rate_imputed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge in plant locations\n",
    "plants = pd.read_excel('Data/eia860/2___Plant_Y2017.xlsx', skiprows=1)\n",
    "plants = plants[['Plant Code', 'Longitude', 'Latitude']]\n",
    "plants.columns = ['EIA_PLANT_ID', 'Longitude', 'Latitude']\n",
    "\n",
    "existing_gen_units = pd.merge(existing_gen_units, plants, on=['EIA_PLANT_ID'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now think about emissions from new sources\n",
    "All new natural gas plants show up in generation files with technology as '_naturalgas_' verus '_natural_gas'\n",
    "Challenge here is to determine how much they emit, and *where*\n",
    "\n",
    "First I make some plots that show that for most pollutants, emissions per unit of energy have gone down a lot in the past thirty years, but that progress really started petering out between 5-10 years ago.  Therefore, I'm going to take technology averages **for the last five years (2016-2020)**\n",
    "\n",
    "Then what about where?  In the ex-post analysis code, I put new facilities at retired sites first (up to previous capacity), then equally distribute capacity to retired/nonretired sites based on capacity.  In this new world where everything is endogenized, this doesn't really make sense since capacity is endogenous!  Moreover in typical scenarios, you'll see a lot more new capacity than retired capacity so it doesn't really make sense to put, say 200MW of capacity at a site that used to have a 25MW capacity.\n",
    "\n",
    "This actually makes things easier though. For all new capacity, I'll just distribute it based on capacity to *all* sites (operating and retired)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\4141580381.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  appendrows['technology'] = 'Natural Gas Fired Combined Cycle With CCS'\n",
      "C:\\Users\\lbeatty\\AppData\\Local\\Temp\\ipykernel_25164\\4141580381.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  appendrows[ratecolumns]=appendrows[ratecolumns].apply(lambda x: x * ccspenalty)\n"
     ]
    }
   ],
   "source": [
    "## Calculate emissions rates for new sources\n",
    "generators_data_2030['planning_year']=2030\n",
    "generators_data_2040['planning_year']=2040\n",
    "generators_data_2050['planning_year']=2050\n",
    "generators_data = pd.concat([generators_data_2030, generators_data_2040, generators_data_2050])\n",
    "\n",
    "\n",
    "## Calculate rates for new sources\n",
    "existing_gen_units['operating_date']=pd.to_datetime(existing_gen_units['operating_date'])\n",
    "technology_rates_newsources = existing_gen_units[existing_gen_units['operating_date']>=pd.to_datetime('2016-01-01')].groupby(['technology', 'planning_year']).apply(weighted_average).reset_index()\n",
    "\n",
    "#Add in CCS\n",
    "# It looks like all new NGCT has heat rate of 6.36 and all new CCCCS has heat rate of 7.16\n",
    "# For now, I'm just going to run with this\n",
    "appendrows = technology_rates_newsources[technology_rates_newsources['technology']=='Natural Gas Fired Combined Cycle']\n",
    "appendrows['technology'] = 'Natural Gas Fired Combined Cycle With CCS'\n",
    "\n",
    "ccspenalty = 7.16/6.36\n",
    "\n",
    "ratecolumns=['nox_rate', 'so2_rate', 'pm25_rate', 'nh3_rate', 'voc_rate']\n",
    "appendrows[ratecolumns]=appendrows[ratecolumns].apply(lambda x: x * ccspenalty)\n",
    "technology_rates_newsources = pd.concat([technology_rates_newsources, appendrows])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now deal with new generation\n",
    "#Isolate retired sites -- dates and \n",
    "new_sites = eia860.dropna(subset=[\"Technology\"])\n",
    "new_sites = new_sites[new_sites['Technology'].str.contains('Natural Gas')]\n",
    "new_sites = pd.merge(new_sites, plants, how='left', on='EIA_PLANT_ID')\n",
    "ipm_regions = gpd.read_file('Data/IPM_Regions/national_emm_boundaries.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make retired sites a geopandas\n",
    "geometry = [Point(lon, lat) for lon, lat in zip(new_sites['Longitude'], new_sites['Latitude'])]\n",
    "new_sites = gpd.GeoDataFrame(new_sites, geometry=geometry, crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lbeatty\\AppData\\Local\\miniforge3\\envs\\powergenome\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3490: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "#ipm_regions['region'] = ipm_regions['IPM_Region'].map({region: val for val, regions in cost_multiplier_region_map.items() for region in regions})\n",
    "ipm_regions = ipm_regions.to_crs('EPSG:4326')\n",
    "\n",
    "new_sites = gpd.sjoin(new_sites, ipm_regions, how=\"left\", op=\"intersects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sites_df = pd.DataFrame(new_sites)\n",
    "new_sites_df = new_sites_df.dropna(subset=['model_regi'])\n",
    "\n",
    "# look for plants matched with multiple model regions\n",
    "# For now I'm just going to take the top row, which will lead to a couple inconsistencies\n",
    "#hopefully the shapefiles will become in better shape\n",
    "new_sites_df = new_sites_df.groupby(['EIA_GENERATOR_ID', 'EIA_PLANT_ID']).first().reset_index()\n",
    "\n",
    "# #want to distribute new capacity evenly to retired sites\n",
    "# #be agnostic about type of natural gas plant -- eg new cc capacity can go at an old ct site\n",
    "new_sites_total= new_sites_df.groupby(['model_regi']).agg({'Capacity':'sum'}).reset_index()\n",
    "new_sites_total.columns = ['model_regi', 'Tot_Capacity']\n",
    "\n",
    "new_sites_df = pd.merge(new_sites_df, new_sites_total, how='left', on=['model_regi'])\n",
    "new_sites_df['pct_total']=new_sites_df['Capacity']/new_sites_df['Tot_Capacity']\n",
    "\n",
    "new_sites_df = new_sites_df[['pct_total', 'Longitude', 'Latitude']]\n",
    "newgenerators = generators_data[generators_data['Resource'].str.contains('naturalgas_')]\n",
    "newgenerators.loc[newgenerators['Resource'].str.contains('_ccavg'), 'technology']= 'Natural Gas Fired Combined Cycle'\n",
    "newgenerators.loc[newgenerators['Resource'].str.contains('_ctavg'), 'technology']= 'Natural Gas Fired Combustion Turbine'\n",
    "newgenerators.loc[newgenerators['Resource'].str.contains('_ccccsavg'), 'technology']= 'Natural Gas Fired Combined Cycle With CCS'\n",
    "\n",
    "newgenerators = pd.merge(newgenerators, technology_rates_newsources, how='left', on=['technology', 'planning_year'])\n",
    "newgenerators = newgenerators.merge(new_sites_df, how=\"cross\")\n",
    "\n",
    "newgenerators['nox_predicted']=newgenerators['pct_total']*newgenerators['nox_rate']\n",
    "newgenerators['so2_predicted']=newgenerators['pct_total']*newgenerators['so2_rate']\n",
    "newgenerators['pm25_predicted']=newgenerators['pct_total']*newgenerators['pm25_rate']\n",
    "newgenerators['voc_predicted']=newgenerators['pct_total']*newgenerators['voc_rate']\n",
    "newgenerators['nh3_predicted']=newgenerators['pct_total']*newgenerators['nh3_rate']\n",
    "\n",
    "newgenerators = newgenerators[['Longitude', 'Latitude', 'Resource', 'planning_year', 'nox_predicted', 'so2_predicted', 'pm25_predicted', 'voc_predicted', 'nh3_predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1307826\n",
      "401978\n"
     ]
    }
   ],
   "source": [
    "#bind all emissions sources together\n",
    "\n",
    "existing_gen_units = existing_gen_units[['Longitude', 'Latitude', 'Resource', 'planning_year', 'nox_predicted', 'so2_predicted', 'pm25_predicted', 'voc_predicted', 'nh3_predicted']]\n",
    "emissions=pd.concat([existing_gen_units, newgenerators], ignore_index=True)\n",
    "\n",
    "print(emissions.shape[0])\n",
    "#aggregate emissions in same spot/cluster\n",
    "emissions=emissions.groupby(['Longitude', 'Latitude', 'Resource', 'planning_year']).agg({'nox_predicted':'sum', 'so2_predicted':'sum', 'pm25_predicted':'sum', 'voc_predicted':'sum', 'nh3_predicted':'sum'}).reset_index()\n",
    "print(emissions.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split the emissions output by planning year and write to shapefiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "## 2030\n",
    "#####\n",
    "emissions_2030 = emissions.query('planning_year==2030')\n",
    "\n",
    "emissions_2030 = emissions_2030[['Longitude', 'Latitude', 'nox_predicted', 'so2_predicted', 'pm25_predicted', 'voc_predicted', 'nh3_predicted', 'Resource']]\n",
    "emissions_2030.columns = ['Longitude', 'Latitude', 'NOx', 'SOx', 'PM2_5', 'VOC', 'NH3', 'Resource']\n",
    "\n",
    "emissions_2030 = gpd.GeoDataFrame(\n",
    "    emissions_2030, geometry = gpd.points_from_xy(emissions_2030.Longitude, emissions_2030.Latitude), crs='EPSG:4326')\n",
    "\n",
    "emissions_2030 = emissions_2030.dropna().reset_index(drop=True)\n",
    "\n",
    "emissions_2030.to_file(filename='InMap/MIP_Emissions/marginal_emissions_2030.shp')\n",
    "\n",
    "#####\n",
    "## 2040\n",
    "#####\n",
    "\n",
    "emissions_2040 = emissions.query('planning_year==2040')\n",
    "\n",
    "emissions_2040 = emissions_2040[['Longitude', 'Latitude', 'nox_predicted', 'so2_predicted', 'pm25_predicted', 'voc_predicted', 'nh3_predicted','Resource']]\n",
    "emissions_2040.columns = ['Longitude', 'Latitude', 'NOx', 'SOx', 'PM2_5', 'VOC', 'NH3', 'Resource']\n",
    "\n",
    "emissions_2040 = gpd.GeoDataFrame(\n",
    "    emissions_2040, geometry = gpd.points_from_xy(emissions_2040.Longitude, emissions_2040.Latitude), crs='EPSG:4326')\n",
    "\n",
    "emissions_2040 = emissions_2040.dropna().reset_index(drop=True)\n",
    "\n",
    "emissions_2040.to_file(filename='InMap/MIP_Emissions/marginal_emissions_2040.shp')\n",
    "\n",
    "#####\n",
    "## 2050\n",
    "#####\n",
    "\n",
    "emissions_2050 = emissions.query('planning_year==2050')\n",
    "\n",
    "emissions_2050 = emissions_2050[['Longitude', 'Latitude', 'nox_predicted', 'so2_predicted', 'pm25_predicted', 'voc_predicted', 'nh3_predicted', 'Resource']]\n",
    "emissions_2050.columns = ['Longitude', 'Latitude', 'NOx', 'SOx', 'PM2_5', 'VOC', 'NH3', 'Resource']\n",
    "\n",
    "emissions_2050 = gpd.GeoDataFrame(\n",
    "    emissions_2050, geometry = gpd.points_from_xy(emissions_2050.Longitude, emissions_2050.Latitude), crs='EPSG:4326')\n",
    "\n",
    "emissions_2050 = emissions_2050.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "emissions_2050.to_file(filename='InMap/MIP_Emissions/marginal_emissions_2050.shp')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
